{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import click\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import os\n",
    "from google.cloud import storage\n",
    "import pyspark\n",
    "import psycopg2\n",
    "from pyspark.sql import SparkSession, types\n",
    "from pyspark.sql import functions as F\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@click.command() # click commands instead of argparse.ArgumentParser()... or sys.argv[n]\n",
    "#@click.option('--sa_path', help='Path to the service account json file')\n",
    "#@click.option('--project_id', help='Project ID of you GCP project')\n",
    "#@click.option('--year', default=2021, help='Year to download')\n",
    "#@click.option('--bucket', help='Name of the bucket to upload the data')\n",
    "#@click.option('--color', help='Str of the taxi-color for which data should be extracted')\n",
    "#@click.option('--month', help='Int of the month to summarize the data for')\n",
    "\n",
    "sa_path = '../mle-neue-fische-gunnaroeh-0fc41b31bc57.json'\n",
    "project_id = 'mle-neue-fische-gunnaroeh'\n",
    "bucket = \"01_data_pipeline_project\" \n",
    "project_id = \"mle-neue-fische-gunnaroeh\" \n",
    "color = \"green\"\n",
    "year = 2021\n",
    "month = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(sa_path, bucket, color, year, month):\n",
    "    # Create Spark Session\n",
    "    # config(\"spark.jars\", \"../postgresql-42.6.0.jar\") \\\n",
    "    spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(f\"Pipe-{color}_taxi_{year}-{month:02d}\") \\\n",
    "    .getOrCreate()\n",
    "    # string of the file to be loaded\n",
    "    file_name = f\"ny_taxi/{color}_tripdata_{year}-{month:02d}.parquet\"\n",
    "    \n",
    "    # Establish connection to GCS-Bucket\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = sa_path\n",
    "    # Create an instance of the GCS client to communicate with the Cloud\n",
    "    client = storage.Client()\n",
    "    \n",
    "    # Retrieve address/path to the specified bucket and a blob representing the table\n",
    "    bucket = client.get_bucket(bucket)\n",
    "    blob = bucket.get_blob(file_name)\n",
    "\n",
    "    # Download parquet and write it to memory as binary to be accessible\n",
    "    pq_taxi = blob.download_as_bytes()    \n",
    "    pq_taxi = BytesIO(pq_taxi)\n",
    "    \n",
    "    # read the object in memory as df -> spark-df\n",
    "    df_taxi = pd.read_parquet(pq_taxi)\n",
    "    df_taxi.drop(\"ehail_fee\", inplace=True, axis=1)\n",
    "    df_taxi = spark.createDataFrame(df_taxi)\n",
    "\n",
    "    for col in df_taxi.columns:\n",
    "        if df_taxi.schema[col].dataType == types.DoubleType():\n",
    "            df_taxi = df_taxi.withColumn(col, F.col(col).cast('float'))\n",
    "\n",
    "    return df_taxi, spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/10 15:10:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "df_taxi, spark = extract_data(sa_path, bucket, color, year, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/10 15:10:30 WARN TaskSetManager: Stage 0 contains a task of very large size (10889 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/07/10 15:10:35 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 0 (TID 0): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------------------+------------+------------+---------+--------------------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------------------+------------+------------+---------+--------------------+\n",
      "|       2| 2021-01-01 00:15:56|  2021-01-01 00:19:52|                 N|       1.0|          43|         151|            1.0|         1.01|        5.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         6.8|         2.0|      1.0|                 0.0|\n",
      "|       2| 2021-01-01 00:25:59|  2021-01-01 00:34:44|                 N|       1.0|         166|         239|            1.0|         2.53|       10.0|  0.5|    0.5|      2.81|         0.0|                  0.3|       16.86|         1.0|      1.0|                2.75|\n",
      "|       2| 2021-01-01 00:45:57|  2021-01-01 00:51:55|                 N|       1.0|          41|          42|            1.0|         1.12|        6.0|  0.5|    0.5|       1.0|         0.0|                  0.3|         8.3|         1.0|      1.0|                 0.0|\n",
      "|       2| 2020-12-31 23:57:51|  2021-01-01 00:04:56|                 N|       1.0|         168|          75|            1.0|         1.99|        8.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         9.3|         2.0|      1.0|                 0.0|\n",
      "|       2| 2021-01-01 00:16:36|  2021-01-01 00:16:40|                 N|       2.0|         265|         265|            3.0|          0.0|      -52.0|  0.0|   -0.5|       0.0|         0.0|                 -0.3|       -52.8|         3.0|      1.0|                 0.0|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------------------+------------+------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_taxi.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df_taxi, spark):\n",
    "    # some sql commands\n",
    "    df_taxi = df_taxi \\\n",
    "        .withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime') \\\n",
    "        .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')\\\n",
    "        .withColumnRenamed('PULocationID', 'pickup_location_id')\\\n",
    "        .withColumnRenamed('DOLocationID', 'dropoff_location_id')\n",
    "    # Temporary SQL Table to be queried\n",
    "    df_taxi.registerTempTable('df_taxi_temp')\n",
    "    # Query the revenue\n",
    "    df_result = spark.sql(\"\"\"\n",
    "                    SELECT pickup_location_id AS revenue_zone,\n",
    "                    date_trunc('day', pickup_datetime) AS revenue_day,\n",
    "                    date_trunc('month', pickup_datetime) AS month,  \n",
    "                    SUM(fare_amount) AS revenue_daily_fare,\n",
    "                    SUM(extra) AS revenue_daily_extra,\n",
    "                    SUM(mta_tax) AS revenue_daily_mta_tax,\n",
    "                    SUM(tip_amount) AS revenue_daily_tip_amount,\n",
    "                    SUM(tolls_amount) AS revenue_daily_tolls_amount,\n",
    "                    SUM(improvement_surcharge) AS revenue_daily_improvement_surcharge,\n",
    "                    SUM(total_amount) AS revenue_daily_total_amount,\n",
    "                    SUM(congestion_surcharge) AS revenue_daily_congestion_surcharge,\n",
    "                    AVG(passenger_count) AS avg_daily_passenger_count,\n",
    "                    AVG(trip_distance) AS avg_daily_trip_distance\n",
    "                    FROM df_taxi_temp\n",
    "                    GROUP BY revenue_zone, revenue_day, month;\n",
    "                      \"\"\")\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gunnaroeh/neuefische/mle-data-pipeline-project/.venv/lib/python3.11/site-packages/pyspark/sql/dataframe.py:330: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_transformed = transform_data(df_taxi, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/10 15:10:37 WARN TaskSetManager: Stage 1 contains a task of very large size (10889 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-------------------+------------------+-------------------+---------------------+------------------------+--------------------------+-----------------------------------+--------------------------+----------------------------------+-------------------------+-----------------------+\n",
      "|revenue_zone|        revenue_day|              month|revenue_daily_fare|revenue_daily_extra|revenue_daily_mta_tax|revenue_daily_tip_amount|revenue_daily_tolls_amount|revenue_daily_improvement_surcharge|revenue_daily_total_amount|revenue_daily_congestion_surcharge|avg_daily_passenger_count|avg_daily_trip_distance|\n",
      "+------------+-------------------+-------------------+------------------+-------------------+---------------------+------------------------+--------------------------+-----------------------------------+--------------------------+----------------------------------+-------------------------+-----------------------+\n",
      "|          68|2021-01-09 00:00:00|2021-01-01 00:00:00|24.200000762939453|                0.0|                  0.5|                     0.0|                       0.0|                0.30000001192092896|                      25.0|                               0.0|                      1.0|                    0.0|\n",
      "|         130|2021-01-13 00:00:00|2021-01-01 00:00:00| 741.9799995422363|               10.5|                 12.5|       87.30999970436096|        12.239999771118164|                 12.300000488758087|         876.8300092220306|                               NaN|                      NaN|     3.8907317378353783|\n",
      "|          49|2021-01-13 00:00:00|2021-01-01 00:00:00| 408.1200008392334|                1.0|                  2.5|                   46.75|                       0.0|                  5.400000214576721|         463.7700004577637|                               NaN|                      NaN|      4.360500042140484|\n",
      "|          37|2021-01-13 00:00:00|2021-01-01 00:00:00| 484.5999994277954|                1.0|                  0.5|                   46.75|        12.239999771118164|                  5.400000214576721|         550.4900064468384|                               NaN|                      NaN|        5.1838887863689|\n",
      "|          33|2021-01-14 00:00:00|2021-01-01 00:00:00| 767.5699996948242|                6.5|                 12.0|       83.75999987125397|                       0.0|                   11.4000004529953|         908.7299938201904|                               NaN|                      NaN|      4.300263180152366|\n",
      "+------------+-------------------+-------------------+------------------+-------------------+---------------------+------------------------+--------------------------+-----------------------------------+--------------------------+----------------------------------+-------------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. L: Load Data onto local Machine PostgreSQL\n",
    "def load_data(df_transformed, spark, color, year, month):\n",
    "    load_dotenv()\n",
    "    # get the Database Credentials\n",
    "    user = os.getenv('USER')\n",
    "    pw = os.getenv('PASSWORD')\n",
    "    host = os.getenv('HOST')\n",
    "    port = os.getenv('PORT')\n",
    "    db = os.getenv('DB')\n",
    "    schema = os.getenv('SCHEMA')\n",
    "    # some commands to write it into storage in the db\n",
    "    # Pyspark unfortunately not working\n",
    "    engine = create_engine(f'postgresql://{user}:{pw}@{host}:{port}/{db}')\n",
    "\n",
    "    table_name = f\"{color}_revenue_{year}_{month}\"\n",
    "    # Write DataFrame to PostgreSQL\n",
    "    # df_transformed.write.format(\"jdbc\") \\\n",
    "    #    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    #    .option(\"schema\", schema) \\\n",
    "    #    .option(\"dbtable\", table_name) \\\n",
    "    #    .option(\"user\", user) \\\n",
    "    #    .option(\"password\", pw) \\\n",
    "    #    .save()\n",
    "    df_transformed = df_transformed.toPandas()\n",
    "    df_transformed.to_sql(name=table_name, con=engine, schema=schema, \n",
    "                          if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'psycopg2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m load_data(df_transformed, spark, color, year, month)\n",
      "Cell \u001b[0;32mIn[11], line 13\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(df_transformed, spark, color, year, month)\u001b[0m\n\u001b[1;32m     10\u001b[0m schema \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mgetenv(\u001b[39m'\u001b[39m\u001b[39mSCHEMA\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[39m# some commands to write it into storage in the db\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m# Pyspark unfortunately not working\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m engine \u001b[39m=\u001b[39m create_engine(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpostgresql://\u001b[39;49m\u001b[39m{\u001b[39;49;00muser\u001b[39m}\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m{\u001b[39;49;00mpw\u001b[39m}\u001b[39;49;00m\u001b[39m@\u001b[39;49m\u001b[39m{\u001b[39;49;00mhost\u001b[39m}\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m{\u001b[39;49;00mport\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mdb\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     15\u001b[0m table_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcolor\u001b[39m}\u001b[39;00m\u001b[39m_revenue_\u001b[39m\u001b[39m{\u001b[39;00myear\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mmonth\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[39m# Write DataFrame to PostgreSQL\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# df_transformed.write.format(\"jdbc\") \\\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m#    .option(\"driver\", \"org.postgresql.Driver\") \\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m#    .option(\"password\", pw) \\\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m#    .save()\u001b[39;00m\n",
      "File \u001b[0;32m<string>:2\u001b[0m, in \u001b[0;36mcreate_engine\u001b[0;34m(url, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/neuefische/mle-data-pipeline-project/.venv/lib/python3.11/site-packages/sqlalchemy/util/deprecations.py:281\u001b[0m, in \u001b[0;36mdeprecated_params.<locals>.decorate.<locals>.warned\u001b[0;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[39mif\u001b[39;00m m \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m    275\u001b[0m         _warn_with_version(\n\u001b[1;32m    276\u001b[0m             messages[m],\n\u001b[1;32m    277\u001b[0m             versions[m],\n\u001b[1;32m    278\u001b[0m             version_warnings[m],\n\u001b[1;32m    279\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[1;32m    280\u001b[0m         )\n\u001b[0;32m--> 281\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/neuefische/mle-data-pipeline-project/.venv/lib/python3.11/site-packages/sqlalchemy/engine/create.py:601\u001b[0m, in \u001b[0;36mcreate_engine\u001b[0;34m(url, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m    600\u001b[0m             dbapi_args[k] \u001b[39m=\u001b[39m pop_kwarg(k)\n\u001b[0;32m--> 601\u001b[0m     dbapi \u001b[39m=\u001b[39m dbapi_meth(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdbapi_args)\n\u001b[1;32m    603\u001b[0m dialect_args[\u001b[39m\"\u001b[39m\u001b[39mdbapi\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dbapi\n\u001b[1;32m    605\u001b[0m dialect_args\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mcompiler_linting\u001b[39m\u001b[39m\"\u001b[39m, compiler\u001b[39m.\u001b[39mNO_LINTING)\n",
      "File \u001b[0;32m~/neuefische/mle-data-pipeline-project/.venv/lib/python3.11/site-packages/sqlalchemy/dialects/postgresql/psycopg2.py:676\u001b[0m, in \u001b[0;36mPGDialect_psycopg2.import_dbapi\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimport_dbapi\u001b[39m(\u001b[39mcls\u001b[39m):\n\u001b[0;32m--> 676\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mpsycopg2\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mreturn\u001b[39;00m psycopg2\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'psycopg2'"
     ]
    }
   ],
   "source": [
    "load_data(df_transformed, spark, color, year, month)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
