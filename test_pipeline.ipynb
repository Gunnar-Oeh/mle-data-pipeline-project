{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import click\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import os\n",
    "from google.cloud import storage\n",
    "import pyspark as ps\n",
    "import psycopg2\n",
    "from pyspark.sql import SparkSession, types\n",
    "from pyspark.sql import functions as F\n",
    "#from pyspark.sql.functions import to_timestamp\n",
    "#from pyspark.sql import date_format\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@click.command() # click commands instead of argparse.ArgumentParser()... or sys.argv[n]\n",
    "#@click.option('--sa_path', help='Path to the service account json file')\n",
    "#@click.option('--project_id', help='Project ID of you GCP project')\n",
    "#@click.option('--year', default=2021, help='Year to download')\n",
    "#@click.option('--bucket', help='Name of the bucket to upload the data')\n",
    "#@click.option('--color', help='Str of the taxi-color for which data should be extracted')\n",
    "#@click.option('--month', help='Int of the month to summarize the data for')\n",
    "\n",
    "sa_path = '../mle-neue-fische-gunnaroeh-0fc41b31bc57.json'\n",
    "project_id = 'mle-neue-fische-gunnaroeh'\n",
    "bucket = \"01_data_pipeline_project\" \n",
    "project_id = \"mle-neue-fische-gunnaroeh\" \n",
    "color = \"green\"\n",
    "year = 2021\n",
    "month = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(sa_path, bucket, color, year, month):\n",
    "    # Create Spark Session\n",
    "    # config(\"spark.jars\", \"../postgresql-42.6.0.jar\") \\\n",
    "    spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(f\"Pipe-{color}_taxi_{year}-{month:02d}\") \\\n",
    "    .getOrCreate()\n",
    "    # string of the file to be loaded\n",
    "    file_name = f\"ny_taxi/{color}_tripdata_{year}-{month:02d}.parquet\"\n",
    "    \n",
    "    # Establish connection to GCS-Bucket\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = sa_path\n",
    "    # Create an instance of the GCS client to communicate with the Cloud\n",
    "    client = storage.Client()\n",
    "    \n",
    "    # Retrieve address/path to the specified bucket and a blob representing the table\n",
    "    bucket = client.get_bucket(bucket)\n",
    "    blob = bucket.get_blob(file_name)\n",
    "\n",
    "    # Download parquet and write it to memory as binary to be accessible\n",
    "    pq_taxi = blob.download_as_bytes()    \n",
    "    pq_taxi = BytesIO(pq_taxi)\n",
    "    \n",
    "    # read the object in memory as df -> spark-df\n",
    "    df_taxi = pd.read_parquet(pq_taxi)\n",
    "    df_taxi.drop(\"ehail_fee\", inplace=True, axis=1)\n",
    "    df_taxi = spark.createDataFrame(df_taxi)\n",
    "\n",
    "    for col in df_taxi.columns:\n",
    "        if df_taxi.schema[col].dataType == types.DoubleType():\n",
    "            df_taxi = df_taxi.withColumn(col, F.col(col).cast('float'))\n",
    "\n",
    "    return df_taxi, spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/11 11:31:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "df_taxi, spark = extract_data(sa_path, bucket, color, year, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df_taxi, spark):\n",
    "    # some sql commands\n",
    "    df_taxi = df_taxi \\\n",
    "        .withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime') \\\n",
    "        .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')\\\n",
    "        .withColumnRenamed('PULocationID', 'pickup_location_id')\\\n",
    "        .withColumnRenamed('DOLocationID', 'dropoff_location_id')\n",
    "    # Temporary SQL Table to be queried\n",
    "    df_taxi.registerTempTable('df_taxi_temp')\n",
    "    # Query the revenue\n",
    "    df_result = spark.sql(\"\"\"\n",
    "                    SELECT pickup_location_id AS revenue_zone,\n",
    "                    date_trunc('day', pickup_datetime) AS revenue_day,\n",
    "                    date_trunc('month', pickup_datetime) AS month,  \n",
    "                    SUM(fare_amount) AS revenue_daily_fare,\n",
    "                    SUM(extra) AS revenue_daily_extra,\n",
    "                    SUM(mta_tax) AS revenue_daily_mta_tax,\n",
    "                    SUM(tip_amount) AS revenue_daily_tip_amount,\n",
    "                    SUM(tolls_amount) AS revenue_daily_tolls_amount,\n",
    "                    SUM(improvement_surcharge) AS revenue_daily_improvement_surcharge,\n",
    "                    SUM(total_amount) AS revenue_daily_total_amount,\n",
    "                    SUM(congestion_surcharge) AS revenue_daily_congestion_surcharge,\n",
    "                    AVG(passenger_count) AS avg_daily_passenger_count,\n",
    "                    AVG(trip_distance) AS avg_daily_trip_distance\n",
    "                    FROM df_taxi_temp\n",
    "                    GROUP BY revenue_zone, revenue_day, month;\n",
    "                      \"\"\")\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = transform_data(df_taxi, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. L: Load Data onto local Machine PostgreSQL\n",
    "def load_data(df_transformed, spark, color, year, month):\n",
    "    load_dotenv()\n",
    "    # get the Database Credentials\n",
    "    user = os.getenv('USER')\n",
    "    pw = os.getenv('PASSWORD')\n",
    "    host = os.getenv('HOST')\n",
    "    port = os.getenv('PORT')\n",
    "    db = os.getenv('DB')\n",
    "    schema = os.getenv('SCHEMA')\n",
    "    # some commands to write it into storage in the db\n",
    "    # Pyspark unfortunately not working\n",
    "    engine = create_engine(f'postgresql://{user}:{pw}@{host}:{port}/{db}')\n",
    "\n",
    "    table_name = f\"{color}_revenue_{year}_{month}\"\n",
    "    # Write DataFrame to PostgreSQL\n",
    "    # df_transformed.write.format(\"jdbc\") \\\n",
    "    #    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    #    .option(\"schema\", schema) \\\n",
    "    #    .option(\"dbtable\", table_name) \\\n",
    "    #    .option(\"user\", user) \\\n",
    "    #    .option(\"password\", pw) \\\n",
    "    #    .save()\n",
    "    df_transformed = df_transformed.withColumn(\"revenue_day\", df_transformed.revenue_day.cast(types.StringType())) \\\n",
    "                                .withColumn(\"month\", df_transformed.month.cast(types.StringType()))\n",
    "    \n",
    "    df_transformed = df_transformed.toPandas()\n",
    "    df_transformed[\"revenue_day\"] = pd.to_datetime(df_transformed[\"revenue_day\"])\n",
    "    df_transformed[\"month\"] = pd.to_datetime(df_transformed[\"month\"])\n",
    "    df_transformed.to_sql(name=table_name, con=engine, schema=schema, \n",
    "                          if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data(df_transformed, spark, color, year, month)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
