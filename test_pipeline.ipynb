{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcloud\u001b[39;00m \u001b[39mimport\u001b[39;00m storage\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdotenv\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dotenv\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "import click\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import os\n",
    "from google.cloud import storage\n",
    "import pyspark\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@click.command() # click commands instead of argparse.ArgumentParser()... or sys.argv[n]\n",
    "#@click.option('--sa_path', help='Path to the service account json file')\n",
    "#@click.option('--project_id', help='Project ID of you GCP project')\n",
    "#@click.option('--year', default=2021, help='Year to download')\n",
    "#@click.option('--bucket', help='Name of the bucket to upload the data')\n",
    "#@click.option('--color', help='Str of the taxi-color for which data should be extracted')\n",
    "#@click.option('--month', help='Int of the month to summarize the data for')\n",
    "\n",
    "sa_path = '../mle-neue-fische-gunnaroeh-0fc41b31bc57.json'\n",
    "project_id = 'mle-neue-fische-gunnaroeh'\n",
    "bucket = \"01_data_pipeline_project\" \n",
    "project_id = \"mle-neue-fische-gunnaroeh\" \n",
    "color = \"green\"\n",
    "year = 2021\n",
    "month = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(sa_path, bucket, color, year, month):\n",
    "    # Create Spark Session\n",
    "    spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(f\"Pipe-{color}_taxi_{year}-{month:02d}\") \\\n",
    "    .getOrCreate()\n",
    "    # string of the file to be loaded\n",
    "    file_name = f\"ny_taxi/{color}_tripdata_{year}-{month:02d}.parquet\"\n",
    "    \n",
    "    # Establish connection to GCS-Bucket\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = sa_path\n",
    "    # Create an instance of the GCS client to communicate with the Cloud\n",
    "    client = storage.Client()\n",
    "    \n",
    "    # Retrieve address/path to the specified bucket and a blob representing the table\n",
    "    bucket = client.get_bucket(bucket)\n",
    "    blob = bucket.get_blob(file_name)\n",
    "\n",
    "    # Download parquet and write it to memory as binary to be accessible\n",
    "    pq_taxi = blob.download_as_bytes()    \n",
    "    pq_taxi = BytesIO(pq_taxi)\n",
    "    \n",
    "    # read the object in memory as df -> spark-df\n",
    "    df_taxi = pd.read_parquet(pq_taxi)\n",
    "    df_taxi.drop(\"ehail_fee\", inplace=True, axis=1)\n",
    "    df_taxi = spark.createDataFrame(df_taxi)\n",
    "\n",
    "    for col in df_taxi.columns:\n",
    "        if df_taxi.schema[col].dataType == types.DoubleType():\n",
    "            df_taxi = df_taxi.withColumn(col, F.col(col).cast('float'))\n",
    "\n",
    "    return df_taxi, spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi, spark = extract_data(sa_path, bucket, color, year, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df_taxi, spark):\n",
    "    # some sql commands\n",
    "    df_taxi = df_taxi \\\n",
    "        .withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime') \\\n",
    "        .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')\\\n",
    "        .withColumnRenamed('PULocationID', 'pickup_location_id')\\\n",
    "        .withColumnRenamed('DOLocationID', 'dropoff_location_id')\n",
    "    # Temporary SQL Table to be queried\n",
    "    df_taxi.registerTempTable('df_taxi_temp')\n",
    "    # Query the revenue\n",
    "    df_result = spark.sql(\"\"\"\n",
    "                    SELECT pickup_location_id AS revenue_zone,\n",
    "                    date_trunc('day', pickup_datetime) AS revenue_day,\n",
    "                    date_trunc('month', pickup_datetime) AS month,  \n",
    "                    SUM(fare_amount) AS revenue_daily_fare,\n",
    "                    SUM(extra) AS revenue_daily_extra,\n",
    "                    SUM(mta_tax) AS revenue_daily_mta_tax,\n",
    "                    SUM(tip_amount) AS revenue_daily_tip_amount,\n",
    "                    SUM(tolls_amount) AS revenue_daily_tolls_amount,\n",
    "                    SUM(improvement_surcharge) AS revenue_daily_improvement_surcharge,\n",
    "                    SUM(total_amount) AS revenue_daily_total_amount,\n",
    "                    SUM(congestion_surcharge) AS revenue_daily_congestion_surcharge,\n",
    "                    AVG(passenger_count) AS avg_daily_passenger_count,\n",
    "                    AVG(trip_distance) AS avg_daily_trip_distance\n",
    "                    FROM df_taxi_temp\n",
    "                    GROUP BY revenue_zone, revenue_day, month;\n",
    "                      \"\"\")\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = transform_data(df_taxi, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. L: Load Data onto local Machine PostgreSQL\n",
    "def load_data(df_transformed):\n",
    "    load_dotenv()\n",
    "    # get the Database Credentials\n",
    "    user = os.getenv('USER')\n",
    "    pw = os.getenv('PASSWORD')\n",
    "    host = os.getenv('HOST')\n",
    "    port = os.getenv('PORT')\n",
    "    db = os.getenv('DB')\n",
    "    schema = os.getenv('SCHEMA')\n",
    "    # some commands to write it into storage in the db\n",
    "    table_name = f\"{color}_revenue_{year}_{month}\"\n",
    "    # Write DataFrame to PostgreSQL\n",
    "    df_transformed.write.format(\"jdbc\") \\\n",
    "        .option(\"url\", f\"jdbc:postgresql://{host}:{port}/{db}\") \\\n",
    "        .option(\"schema\", schema)\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .option(\"user\", user) \\\n",
    "        .option(\"password\", password) \\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data(df_transformed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
