{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'date_format' from 'pyspark.sql' (/Users/gunnaroeh/neuefische/mle-data-pipeline-project/.venv/lib/python3.11/site-packages/pyspark/sql/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpsycopg2\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkSession, types\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m functions \u001b[39mas\u001b[39;00m F, date_format\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdotenv\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dotenv\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mio\u001b[39;00m \u001b[39mimport\u001b[39;00m BytesIO\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'date_format' from 'pyspark.sql' (/Users/gunnaroeh/neuefische/mle-data-pipeline-project/.venv/lib/python3.11/site-packages/pyspark/sql/__init__.py)"
     ]
    }
   ],
   "source": [
    "import click\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import os\n",
    "from google.cloud import storage\n",
    "import pyspark\n",
    "import psycopg2\n",
    "from pyspark.sql import SparkSession, types\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import date_format\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@click.command() # click commands instead of argparse.ArgumentParser()... or sys.argv[n]\n",
    "#@click.option('--sa_path', help='Path to the service account json file')\n",
    "#@click.option('--project_id', help='Project ID of you GCP project')\n",
    "#@click.option('--year', default=2021, help='Year to download')\n",
    "#@click.option('--bucket', help='Name of the bucket to upload the data')\n",
    "#@click.option('--color', help='Str of the taxi-color for which data should be extracted')\n",
    "#@click.option('--month', help='Int of the month to summarize the data for')\n",
    "\n",
    "sa_path = '../mle-neue-fische-gunnaroeh-0fc41b31bc57.json'\n",
    "project_id = 'mle-neue-fische-gunnaroeh'\n",
    "bucket = \"01_data_pipeline_project\" \n",
    "project_id = \"mle-neue-fische-gunnaroeh\" \n",
    "color = \"green\"\n",
    "year = 2021\n",
    "month = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(sa_path, bucket, color, year, month):\n",
    "    # Create Spark Session\n",
    "    # config(\"spark.jars\", \"../postgresql-42.6.0.jar\") \\\n",
    "    spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(f\"Pipe-{color}_taxi_{year}-{month:02d}\") \\\n",
    "    .getOrCreate()\n",
    "    # string of the file to be loaded\n",
    "    file_name = f\"ny_taxi/{color}_tripdata_{year}-{month:02d}.parquet\"\n",
    "    \n",
    "    # Establish connection to GCS-Bucket\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = sa_path\n",
    "    # Create an instance of the GCS client to communicate with the Cloud\n",
    "    client = storage.Client()\n",
    "    \n",
    "    # Retrieve address/path to the specified bucket and a blob representing the table\n",
    "    bucket = client.get_bucket(bucket)\n",
    "    blob = bucket.get_blob(file_name)\n",
    "\n",
    "    # Download parquet and write it to memory as binary to be accessible\n",
    "    pq_taxi = blob.download_as_bytes()    \n",
    "    pq_taxi = BytesIO(pq_taxi)\n",
    "    \n",
    "    # read the object in memory as df -> spark-df\n",
    "    df_taxi = pd.read_parquet(pq_taxi)\n",
    "    df_taxi.drop(\"ehail_fee\", inplace=True, axis=1)\n",
    "    df_taxi = spark.createDataFrame(df_taxi)\n",
    "\n",
    "    for col in df_taxi.columns:\n",
    "        if df_taxi.schema[col].dataType == types.DoubleType():\n",
    "            df_taxi = df_taxi.withColumn(col, F.col(col).cast('float'))\n",
    "\n",
    "    return df_taxi, spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/10 15:52:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "df_taxi, spark = extract_data(sa_path, bucket, color, year, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/10 15:10:30 WARN TaskSetManager: Stage 0 contains a task of very large size (10889 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/07/10 15:10:35 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 0 (TID 0): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------------------+------------+------------+---------+--------------------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------------------+------------+------------+---------+--------------------+\n",
      "|       2| 2021-01-01 00:15:56|  2021-01-01 00:19:52|                 N|       1.0|          43|         151|            1.0|         1.01|        5.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         6.8|         2.0|      1.0|                 0.0|\n",
      "|       2| 2021-01-01 00:25:59|  2021-01-01 00:34:44|                 N|       1.0|         166|         239|            1.0|         2.53|       10.0|  0.5|    0.5|      2.81|         0.0|                  0.3|       16.86|         1.0|      1.0|                2.75|\n",
      "|       2| 2021-01-01 00:45:57|  2021-01-01 00:51:55|                 N|       1.0|          41|          42|            1.0|         1.12|        6.0|  0.5|    0.5|       1.0|         0.0|                  0.3|         8.3|         1.0|      1.0|                 0.0|\n",
      "|       2| 2020-12-31 23:57:51|  2021-01-01 00:04:56|                 N|       1.0|         168|          75|            1.0|         1.99|        8.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         9.3|         2.0|      1.0|                 0.0|\n",
      "|       2| 2021-01-01 00:16:36|  2021-01-01 00:16:40|                 N|       2.0|         265|         265|            3.0|          0.0|      -52.0|  0.0|   -0.5|       0.0|         0.0|                 -0.3|       -52.8|         3.0|      1.0|                 0.0|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------------------+------------+------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_taxi.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df_taxi, spark):\n",
    "    # some sql commands\n",
    "    df_taxi = df_taxi \\\n",
    "        .withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime') \\\n",
    "        .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')\\\n",
    "        .withColumnRenamed('PULocationID', 'pickup_location_id')\\\n",
    "        .withColumnRenamed('DOLocationID', 'dropoff_location_id')\n",
    "    # Temporary SQL Table to be queried\n",
    "    df_taxi.registerTempTable('df_taxi_temp')\n",
    "    # Query the revenue\n",
    "    df_result = spark.sql(\"\"\"\n",
    "                    SELECT pickup_location_id AS revenue_zone,\n",
    "                    date_trunc('day', pickup_datetime) AS revenue_day,\n",
    "                    date_trunc('month', pickup_datetime) AS month,  \n",
    "                    SUM(fare_amount) AS revenue_daily_fare,\n",
    "                    SUM(extra) AS revenue_daily_extra,\n",
    "                    SUM(mta_tax) AS revenue_daily_mta_tax,\n",
    "                    SUM(tip_amount) AS revenue_daily_tip_amount,\n",
    "                    SUM(tolls_amount) AS revenue_daily_tolls_amount,\n",
    "                    SUM(improvement_surcharge) AS revenue_daily_improvement_surcharge,\n",
    "                    SUM(total_amount) AS revenue_daily_total_amount,\n",
    "                    SUM(congestion_surcharge) AS revenue_daily_congestion_surcharge,\n",
    "                    AVG(passenger_count) AS avg_daily_passenger_count,\n",
    "                    AVG(trip_distance) AS avg_daily_trip_distance\n",
    "                    FROM df_taxi_temp\n",
    "                    GROUP BY revenue_zone, revenue_day, month;\n",
    "                      \"\"\")\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gunnaroeh/neuefische/mle-data-pipeline-project/.venv/lib/python3.11/site-packages/pyspark/sql/dataframe.py:330: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_transformed = transform_data(df_taxi, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'date_format' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_transformed\u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m\"\u001b[39m, date_format(\u001b[39m\"\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39myyyy-MM-dd HH:mm:ss\u001b[39m\u001b[39m\"\u001b[39m))\u001b[39m.\u001b[39mtoPandas()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'date_format' is not defined"
     ]
    }
   ],
   "source": [
    "df_transformed.withColumn(\"timestamp\", date_format(\"timestamp\", \"yyyy-MM-dd HH:mm:ss\")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Casting to unit-less dtype 'datetime64' is not supported. Pass e.g. 'datetime64[ns]' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_transformed\u001b[39m.\u001b[39;49mtoPandas()\n",
      "File \u001b[0;32m~/neuefische/mle-data-pipeline-project/.venv/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:251\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m should_check_timedelta \u001b[39m=\u001b[39m is_timedelta64_dtype(t) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(pdf) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    250\u001b[0m \u001b[39mif\u001b[39;00m (t \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_timedelta64_dtype(t)) \u001b[39mor\u001b[39;00m should_check_timedelta:\n\u001b[0;32m--> 251\u001b[0m     series \u001b[39m=\u001b[39m series\u001b[39m.\u001b[39;49mastype(t, copy\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    253\u001b[0m \u001b[39mwith\u001b[39;00m catch_warnings():\n\u001b[1;32m    254\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39merrors\u001b[39;00m \u001b[39mimport\u001b[39;00m PerformanceWarning\n",
      "File \u001b[0;32m~/neuefische/mle-data-pipeline-project/.venv/lib/python3.11/site-packages/pandas/core/generic.py:6324\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6317\u001b[0m     results \u001b[39m=\u001b[39m [\n\u001b[1;32m   6318\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miloc[:, i]\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n\u001b[1;32m   6319\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns))\n\u001b[1;32m   6320\u001b[0m     ]\n\u001b[1;32m   6322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   6323\u001b[0m     \u001b[39m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6324\u001b[0m     new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mastype(dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   6325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_data)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mastype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6327\u001b[0m \u001b[39m# GH 33113: handle empty frame or series\u001b[39;00m\n",
      "File \u001b[0;32m~/neuefische/mle-data-pipeline-project/.venv/lib/python3.11/site-packages/pandas/core/internals/managers.py:451\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39melif\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    449\u001b[0m     copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(\n\u001b[1;32m    452\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mastype\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    453\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    454\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    455\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    456\u001b[0m     using_cow\u001b[39m=\u001b[39;49musing_copy_on_write(),\n\u001b[1;32m    457\u001b[0m )\n",
      "File \u001b[0;32m~/neuefische/mle-data-pipeline-project/.venv/lib/python3.11/site-packages/pandas/core/internals/managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m         applied \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mapply(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 352\u001b[0m         applied \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(b, f)(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    353\u001b[0m     result_blocks \u001b[39m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    355\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mfrom_blocks(result_blocks, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/neuefische/mle-data-pipeline-project/.venv/lib/python3.11/site-packages/pandas/core/internals/blocks.py:511\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors, using_cow)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[39mCoerce to the new dtype.\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39mBlock\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    509\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues\n\u001b[0;32m--> 511\u001b[0m new_values \u001b[39m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m    513\u001b[0m new_values \u001b[39m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    515\u001b[0m refs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/neuefische/mle-data-pipeline-project/.venv/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:242\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    239\u001b[0m     dtype \u001b[39m=\u001b[39m dtype\u001b[39m.\u001b[39mnumpy_dtype\n\u001b[1;32m    241\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 242\u001b[0m     new_values \u001b[39m=\u001b[39m astype_array(values, dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m    243\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[1;32m    244\u001b[0m     \u001b[39m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[39m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/neuefische/mle-data-pipeline-project/.venv/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:184\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[39mreturn\u001b[39;00m values\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(values, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m    183\u001b[0m     \u001b[39m# i.e. ExtensionArray\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39;49mastype(dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     values \u001b[39m=\u001b[39m _astype_nansafe(values, dtype, copy\u001b[39m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/neuefische/mle-data-pipeline-project/.venv/lib/python3.11/site-packages/pandas/core/arrays/datetimes.py:694\u001b[0m, in \u001b[0;36mDatetimeArray.astype\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    683\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot use .astype to convert from timezone-aware dtype to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    684\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtimezone-naive dtype. Use obj.tz_localize(None) or \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    685\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mobj.tz_convert(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUTC\u001b[39m\u001b[39m'\u001b[39m\u001b[39m).tz_localize(None) instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    688\u001b[0m \u001b[39melif\u001b[39;00m (\n\u001b[1;32m    689\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtz \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     \u001b[39mand\u001b[39;00m is_datetime64_dtype(dtype)\n\u001b[1;32m    691\u001b[0m     \u001b[39mand\u001b[39;00m dtype \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype\n\u001b[1;32m    692\u001b[0m     \u001b[39mand\u001b[39;00m is_unitless(dtype)\n\u001b[1;32m    693\u001b[0m ):\n\u001b[0;32m--> 694\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    695\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCasting to unit-less dtype \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdatetime64\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not supported. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    696\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPass e.g. \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdatetime64[ns]\u001b[39m\u001b[39m'\u001b[39m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    697\u001b[0m     )\n\u001b[1;32m    699\u001b[0m \u001b[39melif\u001b[39;00m is_period_dtype(dtype):\n\u001b[1;32m    700\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_period(freq\u001b[39m=\u001b[39mdtype\u001b[39m.\u001b[39mfreq)\n",
      "\u001b[0;31mTypeError\u001b[0m: Casting to unit-less dtype 'datetime64' is not supported. Pass e.g. 'datetime64[ns]' instead."
     ]
    }
   ],
   "source": [
    "df_transformed.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. L: Load Data onto local Machine PostgreSQL\n",
    "def load_data(df_transformed, spark, color, year, month):\n",
    "    load_dotenv()\n",
    "    # get the Database Credentials\n",
    "    user = os.getenv('USER')\n",
    "    pw = os.getenv('PASSWORD')\n",
    "    host = os.getenv('HOST')\n",
    "    port = os.getenv('PORT')\n",
    "    db = os.getenv('DB')\n",
    "    schema = os.getenv('SCHEMA')\n",
    "    # some commands to write it into storage in the db\n",
    "    # Pyspark unfortunately not working\n",
    "    engine = create_engine(f'postgresql://{user}:{pw}@{host}:{port}/{db}')\n",
    "\n",
    "    table_name = f\"{color}_revenue_{year}_{month}\"\n",
    "    # Write DataFrame to PostgreSQL\n",
    "    # df_transformed.write.format(\"jdbc\") \\\n",
    "    #    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    #    .option(\"schema\", schema) \\\n",
    "    #    .option(\"dbtable\", table_name) \\\n",
    "    #    .option(\"user\", user) \\\n",
    "    #    .option(\"password\", pw) \\\n",
    "    #    .save()\n",
    "    df_transformed = df_transformed.toPandas()\n",
    "    df_transformed.to_sql(name=table_name, con=engine, schema=schema, \n",
    "                          if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/10 15:53:04 WARN TaskSetManager: Stage 0 contains a task of very large size (10889 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Casting to unit-less dtype 'datetime64' is not supported. Pass e.g. 'datetime64[ns]' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m load_data(df_transformed, spark, color, year, month)\n",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(df_transformed, spark, color, year, month)\u001b[0m\n\u001b[1;32m     15\u001b[0m table_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcolor\u001b[39m}\u001b[39;00m\u001b[39m_revenue_\u001b[39m\u001b[39m{\u001b[39;00myear\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mmonth\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[39m# Write DataFrame to PostgreSQL\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# df_transformed.write.format(\"jdbc\") \\\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m#    .option(\"driver\", \"org.postgresql.Driver\") \\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m#    .option(\"password\", pw) \\\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m#    .save()\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m df_transformed \u001b[39m=\u001b[39m df_transformed\u001b[39m.\u001b[39;49mtoPandas()\n\u001b[1;32m     25\u001b[0m df_transformed\u001b[39m.\u001b[39mto_sql(name\u001b[39m=\u001b[39mtable_name, con\u001b[39m=\u001b[39mengine, schema\u001b[39m=\u001b[39mschema, \n\u001b[1;32m     26\u001b[0m                       if_exists\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreplace\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/neuefische/mle-data-pipeline-project/.venv/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:251\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m should_check_timedelta \u001b[39m=\u001b[39m is_timedelta64_dtype(t) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(pdf) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    250\u001b[0m \u001b[39mif\u001b[39;00m (t \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_timedelta64_dtype(t)) \u001b[39mor\u001b[39;00m should_check_timedelta:\n\u001b[0;32m--> 251\u001b[0m     series \u001b[39m=\u001b[39m series\u001b[39m.\u001b[39;49mastype(t, copy\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    253\u001b[0m \u001b[39mwith\u001b[39;00m catch_warnings():\n\u001b[1;32m    254\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39merrors\u001b[39;00m \u001b[39mimport\u001b[39;00m PerformanceWarning\n",
      "File \u001b[0;32m~/neuefische/mle-data-pipeline-project/.venv/lib/python3.11/site-packages/pandas/core/generic.py:6324\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6317\u001b[0m     results \u001b[39m=\u001b[39m [\n\u001b[1;32m   6318\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miloc[:, i]\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n\u001b[1;32m   6319\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns))\n\u001b[1;32m   6320\u001b[0m     ]\n\u001b[1;32m   6322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   6323\u001b[0m     \u001b[39m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6324\u001b[0m     new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mastype(dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   6325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_data)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mastype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6327\u001b[0m \u001b[39m# GH 33113: handle empty frame or series\u001b[39;00m\n",
      "File \u001b[0;32m~/neuefische/mle-data-pipeline-project/.venv/lib/python3.11/site-packages/pandas/core/internals/managers.py:451\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39melif\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    449\u001b[0m     copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(\n\u001b[1;32m    452\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mastype\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    453\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    454\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    455\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    456\u001b[0m     using_cow\u001b[39m=\u001b[39;49musing_copy_on_write(),\n\u001b[1;32m    457\u001b[0m )\n",
      "File \u001b[0;32m~/neuefische/mle-data-pipeline-project/.venv/lib/python3.11/site-packages/pandas/core/internals/managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m         applied \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mapply(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 352\u001b[0m         applied \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(b, f)(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    353\u001b[0m     result_blocks \u001b[39m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    355\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mfrom_blocks(result_blocks, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/neuefische/mle-data-pipeline-project/.venv/lib/python3.11/site-packages/pandas/core/internals/blocks.py:511\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors, using_cow)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[39mCoerce to the new dtype.\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39mBlock\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    509\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues\n\u001b[0;32m--> 511\u001b[0m new_values \u001b[39m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m    513\u001b[0m new_values \u001b[39m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    515\u001b[0m refs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/neuefische/mle-data-pipeline-project/.venv/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:242\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    239\u001b[0m     dtype \u001b[39m=\u001b[39m dtype\u001b[39m.\u001b[39mnumpy_dtype\n\u001b[1;32m    241\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 242\u001b[0m     new_values \u001b[39m=\u001b[39m astype_array(values, dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m    243\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[1;32m    244\u001b[0m     \u001b[39m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[39m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/neuefische/mle-data-pipeline-project/.venv/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:184\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[39mreturn\u001b[39;00m values\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(values, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m    183\u001b[0m     \u001b[39m# i.e. ExtensionArray\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39;49mastype(dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     values \u001b[39m=\u001b[39m _astype_nansafe(values, dtype, copy\u001b[39m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/neuefische/mle-data-pipeline-project/.venv/lib/python3.11/site-packages/pandas/core/arrays/datetimes.py:694\u001b[0m, in \u001b[0;36mDatetimeArray.astype\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    683\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot use .astype to convert from timezone-aware dtype to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    684\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtimezone-naive dtype. Use obj.tz_localize(None) or \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    685\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mobj.tz_convert(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUTC\u001b[39m\u001b[39m'\u001b[39m\u001b[39m).tz_localize(None) instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    688\u001b[0m \u001b[39melif\u001b[39;00m (\n\u001b[1;32m    689\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtz \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     \u001b[39mand\u001b[39;00m is_datetime64_dtype(dtype)\n\u001b[1;32m    691\u001b[0m     \u001b[39mand\u001b[39;00m dtype \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype\n\u001b[1;32m    692\u001b[0m     \u001b[39mand\u001b[39;00m is_unitless(dtype)\n\u001b[1;32m    693\u001b[0m ):\n\u001b[0;32m--> 694\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    695\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCasting to unit-less dtype \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdatetime64\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not supported. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    696\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPass e.g. \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdatetime64[ns]\u001b[39m\u001b[39m'\u001b[39m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    697\u001b[0m     )\n\u001b[1;32m    699\u001b[0m \u001b[39melif\u001b[39;00m is_period_dtype(dtype):\n\u001b[1;32m    700\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_period(freq\u001b[39m=\u001b[39mdtype\u001b[39m.\u001b[39mfreq)\n",
      "\u001b[0;31mTypeError\u001b[0m: Casting to unit-less dtype 'datetime64' is not supported. Pass e.g. 'datetime64[ns]' instead."
     ]
    }
   ],
   "source": [
    "load_data(df_transformed, spark, color, year, month)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
